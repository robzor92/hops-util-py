{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `hops-util-py` Integration Tests\n",
    "\n",
    "This notebook can be converted to a python file and submitted as a spark job for integration tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>18</td><td>application_1571991823790_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1571991823790_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e03_1571991823790_0002_01_000001/fawfawdasd__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import experiment, hdfs, tensorboard, devices, kafka, featurestore, tls, util, serving, model, constants\n",
    "from hops.experiment import Direction\n",
    "from hops.model import Metric\n",
    "import stat\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, IntegerType, FloatType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "from pyspark.sql import DataFrame\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment API Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_asserts():\n",
    "    from hops import tensorboard\n",
    "    from hops import devices\n",
    "    from hops import hdfs\n",
    "    import os\n",
    "    assert tensorboard.logdir() != None\n",
    "    assert devices.get_num_gpus() == 0\n",
    "    assert hdfs.project_path() == hdfs.project_path(hdfs.project_name())\n",
    "    if tensorboard.local_logdir_bool:\n",
    "        assert \"hdfs://\" not in tensorboard.logdir()\n",
    "        assert os.path.exists(tensorboard.logdir())\n",
    "    else:\n",
    "        assert \"hdfs://\" in tensorboard.logdir()\n",
    "        assert hdfs.exists(tensorboard.logdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_ret():\n",
    "    exp_asserts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_ret_params(a, b):\n",
    "    exp_asserts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ret_raw_value():\n",
    "    exp_asserts()\n",
    "    return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ret_raw_value_params(a, b):\n",
    "    exp_asserts()\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ret_path():\n",
    "    exp_asserts()\n",
    "    f = open('testfile.txt', 'w')\n",
    "    f.write('stuff happened')\n",
    "    f.close()\n",
    "    return {'logfile': 'testfile.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ret_path_params(a, b):\n",
    "    exp_asserts()\n",
    "    f = open('testfile.txt', 'w')\n",
    "    f.write('stuff happened')\n",
    "    f.close()\n",
    "    return {'logfile': 'testfile.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ret_val():\n",
    "    exp_asserts()\n",
    "    return {'value': -10.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ret_val_params(a, b):\n",
    "    exp_asserts()\n",
    "    return {'value': a+b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ret():\n",
    "    exp_asserts()\n",
    "    f = open('testfile.txt', 'w')\n",
    "    f.write('stuff happened')\n",
    "    f.close()\n",
    "    return {'value': 10, 'morevals': 0.5, 'logfile': 'testfile.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ret_params(a, b):\n",
    "    exp_asserts()\n",
    "    f = open('testfile.txt', 'w')\n",
    "    f.write('stuff happened')\n",
    "    f.close()\n",
    "    return {'value': a+b, 'morevals': b, 'logfile': 'testfile.txt', 'diagram': 'img.png'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_in_tensorboard_logdir():\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "    import os\n",
    "    import uuid\n",
    "    model_name = str(uuid.uuid4())\n",
    "    \n",
    "    if tensorboard.logdir():\n",
    "        if os.path.exists(tensorboard.logdir()):\n",
    "            os.mkdir(tensorboard.logdir() + '/model')\n",
    "            f = open(tensorboard.logdir() + '/model/model.pb', 'w')\n",
    "            f.write('model')\n",
    "            f.close()\n",
    "        else:\n",
    "            hdfs.mkdir(tensorboard.logdir() + '/model')\n",
    "            hdfs.dump(\"model\", tensorboard.logdir() + '/model/model.pb')\n",
    "    \n",
    "    return {'name': model_name}\n",
    "\n",
    "def create_model_in_tensorboard_logdir_params(a, b):\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "    import os\n",
    "    import uuid\n",
    "    model_name = str(uuid.uuid4())\n",
    "    model_path = tensorboard.logdir() + '/model/' + model_name\n",
    "    \n",
    "    #create a 'model'\n",
    "    if tensorboard.logdir():\n",
    "        if os.path.exists(tensorboard.logdir()):\n",
    "            os.mkdir(tensorboard.logdir() + '/model')\n",
    "            f = open(tensorboard.logdir() + '/model/model.pb', 'w')\n",
    "            f.write('model')\n",
    "            f.close()\n",
    "        else:\n",
    "            hdfs.mkdir(tensorboard.logdir() + '/model')\n",
    "            hdfs.dump(\"model\", tensorboard.logdir() + '/model/model.pb')\n",
    "    \n",
    "    return {'name': model_name, 'optval': a+b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_in_wrapper():\n",
    "    ret_dict = create_model_in_tensorboard_logdir()\n",
    "    from hops import model\n",
    "    from hops import tensorboard\n",
    "    if tensorboard.logdir():\n",
    "        model.export(tensorboard.logdir() + '/model', ret_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_return_values(logdir, hp_dict, should_return_hp_dict, return_dict, should_return_return_dict):\n",
    "    assert hdfs.exists(logdir)\n",
    "    \n",
    "    if should_return_hp_dict:\n",
    "        print('Asserting hp_dict {} is a dict'.format(hp_dict))\n",
    "        assert type(hp_dict) == dict\n",
    "    else:\n",
    "        assert not hp_dict\n",
    "        \n",
    "    if should_return_return_dict:\n",
    "        print('Asserting return_dict {} is a dict'.format(return_dict))\n",
    "        assert type(return_dict) == dict    \n",
    "    else:\n",
    "        assert not return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test `experiment.launch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "params={'a': [-5, 4.9], 'b': [-8, 10.3]}\n",
    "\n",
    "logdir, return_dict = experiment.launch(no_ret, local_logdir=False, name='no ret')\n",
    "assert_return_values(logdir, None, False, return_dict, False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(no_ret, local_logdir=True, name='no ret')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(no_ret_params, params, local_logdir=True, name='no ret params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(no_ret_params, params, local_logdir=False, name='no ret params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'metric': '10'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_raw_value, local_logdir=False, name='single ret raw value')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'metric': '10'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_raw_value, local_logdir=True, name='single ret raw value')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_raw_value_params, params, local_logdir=True, name='single ret raw value params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_raw_value_params, params, local_logdir=False, name='single ret raw value params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'logfile': 'Experiments/application_1571904075501_0011_9/testfile.txt'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_path, local_logdir=False, description='some custom desc', name='single ret path')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'logfile': 'Experiments/application_1571904075501_0011_10/testfile.txt'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_path, local_logdir=True, name='single ret path')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_path_params, params, local_logdir=True, name='single ret path params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_path_params, params, local_logdir=False, name='single ret path params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'value': '-10.3'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_val, local_logdir=False, name='single ret val', description='some custom desc')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'value': '-10.3'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_val, local_logdir=True, name='single ret val')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_val_params, params, local_logdir=True, name='single ret val params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(single_ret_val_params, params, local_logdir=False, name='single ret val params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'value': '10', 'morevals': '0.5', 'logfile': 'Experiments/application_1571904075501_0011_17/testfile.txt'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(multi_ret, local_logdir=False, name='multi ret', metric_key='value')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'value': '10', 'morevals': '0.5', 'logfile': 'Experiments/application_1571904075501_0011_18/testfile.txt'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(multi_ret, local_logdir=True, name='multi ret', metric_key='morevals')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(multi_ret_params, params, local_logdir=False, name='multi ret params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(multi_ret_params, params, local_logdir=True, name='multi ret params')\n",
    "assert_return_values(logdir, None, False, return_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'metric': '-13'} is a dict"
     ]
    }
   ],
   "source": [
    "params={'a': [-5], 'b': [-8]}\n",
    "\n",
    "logdir, return_dict = experiment.launch(single_ret_raw_value_params, params, local_logdir=True, name='multi ret params single comb')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Asserting return_dict {'value': '-13', 'morevals': '-8', 'logfile': 'Experiments/application_1571904075501_0011_22/a=-5&b=-8/testfile.txt', 'diagram': 'img.png'} is a dict"
     ]
    }
   ],
   "source": [
    "logdir, return_dict = experiment.launch(multi_ret_params, params, local_logdir=True, name='multi ret params single comb', metric_key='value')\n",
    "assert_return_values(logdir, None, False, return_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "Finished Experiment \n",
      "\n",
      "Finished Experiment \n",
      "\n",
      "Exported model 5a446f03-421e-4aae-bbd7-1a4ae98e1c13 as version 1 successfully.\n",
      "Polling 5a446f03-421e-4aae-bbd7-1a4ae98e1c13 version 1 for model availability.\n",
      "Model now available.\n",
      "Finished Experiment \n",
      "\n",
      "Exported model fee7334d-f5b9-4381-8139-a2338a89cf29 as version 1 successfully.\n",
      "Polling fee7334d-f5b9-4381-8139-a2338a89cf29 version 1 for model availability.\n",
      "Model now available."
     ]
    }
   ],
   "source": [
    "experiment.launch(export_model_in_wrapper, name='model exported in wrapper', local_logdir=False)\n",
    "\n",
    "experiment.launch(export_model_in_wrapper, name='model exported in wrapper', local_logdir=True)\n",
    "\n",
    "logdir, return_dict = experiment.launch(create_model_in_tensorboard_logdir, local_logdir=True, name='model exported from local logdir')\n",
    "model.export(logdir + '/model', return_dict['name'])\n",
    "\n",
    "logdir, return_dict = experiment.launch(create_model_in_tensorboard_logdir, local_logdir=False, name='model exported from hdfs logdir')\n",
    "model.export(logdir + '/model', return_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_best_hyperparameters(return_dict, best_hyperparameters):\n",
    "    print('Asserting best hyperparameters in return_dict {} are {}'.format(return_dict, best_hyperparameters))\n",
    "    for key in best_hyperparameters.keys():\n",
    "        assert float(best_hyperparameters[key]) == float(return_dict[key]), '{} not equal to {}'.format(best_hyperparameters[key], return_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_return_dict(logdir, return_dict):\n",
    "    return_dict_contents = hdfs.load(logdir + '/.outputs.json')\n",
    "    logdir_return_dict = json.loads(return_dict_contents)\n",
    "    print('Assserting returned dict {} is equal to .return in best logdir {}'.format(return_dict, logdir_return_dict))\n",
    "    assert return_dict == logdir_return_dict, 'dicts are not the same {} - {}'.format(return_dict, logdir_return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Parallel Experiments `experiment.grid_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://10.0.2.15:8998/sessions/16/statements/41 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "params={'a': [-5, 4.9], 'b': [-8, 10.3]}\n",
    "try:\n",
    "    experiment.grid_search(no_ret_params, params, name='fail no ret val')\n",
    "    assert False, 'should fail due to no return value'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    experiment.grid_search(multi_ret_params, params, name='fail no opt key')\n",
    "    assert False, 'should fail due to optimization_key not being set'\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "logdir, hp_dict, return_dict = experiment.grid_search(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MIN)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.grid_search(single_ret_raw_value_params, params, local_logdir=False, direction=Direction.MAX)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.grid_search(single_ret_val_params, params, local_logdir=True, direction=Direction.MIN, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.grid_search(single_ret_val_params, params, local_logdir=False, direction=Direction.MAX, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.grid_search(multi_ret_params, params, local_logdir=False, direction=Direction.MIN, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, metric = experiment.grid_search(multi_ret_params, params, local_logdir=True, direction=Direction.MAX, optimization_key='morevals')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "params={'a': [-1, 1.5], 'b': [-1.5, 1]}\n",
    "\n",
    "# Make sure minimization work\n",
    "logdir, hp_dict, return_dict = experiment.grid_search(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MIN)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "assert_best_hyperparameters(hp_dict, {'a': -1, 'b': -1.5})\n",
    "assert_return_dict(logdir, return_dict)\n",
    "\n",
    "# Make sure maximization work\n",
    "logdir, hp_dict, return_dict = experiment.grid_search(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MAX)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "assert_best_hyperparameters(hp_dict, {'a': 1.5, 'b': 1})\n",
    "assert_return_dict(logdir, return_dict)\n",
    "\n",
    "best_logdir, hp_dict, return_dict = experiment.grid_search(create_model_in_tensorboard_logdir_params, params, local_logdir=True, name='grid search model exported from local logdir', optimization_key='optval', direction=Direction.MIN)\n",
    "model.export(best_logdir + '/model', return_dict['name'])\n",
    "\n",
    "best_logdir, hp_dict, return_dict = experiment.grid_search(create_model_in_tensorboard_logdir_params, params, local_logdir=False, name='grid search model exported from hdfs logdir', optimization_key='optval', direction=Direction.MAX)\n",
    "model.export(best_logdir + '/model', return_dict['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Parallel Experiments `experiment.random_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "params={'a': [-5, 4.9], 'b': [-8, 10.3]}\n",
    "try:\n",
    "    experiment.random_search(no_ret_params, params, samples=2, name='fail opt no ret')\n",
    "    assert False, 'should fail due to no return value'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    experiment.random_search(multi_ret_params, params, samples=2, name='fail opt no key')\n",
    "    assert False, 'should fail due to optimization_key not being set'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.random_search(single_ret_raw_value_params, params, samples=2, local_logdir=True, direction=Direction.MIN)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.random_search(single_ret_raw_value_params, params, samples=2, local_logdir=False, direction=Direction.MAX)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.random_search(single_ret_val_params, params, samples=2, local_logdir=True, direction=Direction.MIN, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.random_search(single_ret_val_params, params, samples=2, local_logdir=False, direction=Direction.MAX, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.random_search(multi_ret_params, params, samples=2, local_logdir=False, direction=Direction.MAX, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.random_search(multi_ret_params, params, samples=2, local_logdir=True, direction=Direction.MIN, optimization_key='morevals')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "params={'a': [-1, 1], 'b': [-1, 1]}\n",
    "\n",
    "# Make sure minimization work\n",
    "logdir, hp_dict, return_dict = experiment.random_search(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MIN, samples=100)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "assert_best_hyperparameters(hp_dict, {'a': -1, 'b': -1})\n",
    "assert_return_dict(logdir, return_dict)\n",
    "\n",
    "# Make sure maximization work\n",
    "logdir, hp_dict, return_dict = experiment.random_search(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MAX, samples=100)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "assert_best_hyperparameters(hp_dict, {'a': 1, 'b': 1})\n",
    "assert_return_dict(logdir, return_dict)\n",
    "\n",
    "best_logdir, hp_dict, return_dict = experiment.random_search(create_model_in_tensorboard_logdir_params, params, local_logdir=True, name='random search model exported from local logdir', optimization_key='optval', direction=Direction.MIN)\n",
    "model.export(best_logdir + '/model', return_dict['name'])\n",
    "\n",
    "best_logdir, hp_dict, return_dict = experiment.random_search(create_model_in_tensorboard_logdir_params, params, local_logdir=False, name='random search model exported from hdfs logdir', optimization_key='optval', direction=Direction.MAX)\n",
    "model.export(best_logdir + '/model', return_dict['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Parallel Experiments `experiment.differential_evolution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "params={'a': [1, 4.9], 'b': [3, 10.3]}\n",
    "try:\n",
    "    experiment.differential_evolution(no_ret_params, params, name='fail opt no ret')\n",
    "    assert False, 'should fail due to no return value'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    experiment.differential_evolution(multi_ret_params, params, name='fail opt no key')\n",
    "    assert False, 'should fail due to optimization_key not being set'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MIN)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(single_ret_raw_value_params, params, local_logdir=False, direction=Direction.MAX)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(single_ret_val_params, params, local_logdir=True, direction=Direction.MIN, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(single_ret_val_params, params,local_logdir=False, direction=Direction.MAX, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(multi_ret_params, params, local_logdir=False, direction=Direction.MAX, optimization_key='value')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(multi_ret_params, params, local_logdir=True, direction=Direction.MIN, optimization_key='morevals')\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "\n",
    "params={'a': [4, 5], 'b': [1, 2]}\n",
    "\n",
    "# Make sure minimization work\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MIN)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "assert_best_hyperparameters(hp_dict, {'a': 4, 'b': 1})\n",
    "assert_return_dict(logdir, return_dict)\n",
    "\n",
    "# Make sure maximization work\n",
    "logdir, hp_dict, return_dict = experiment.differential_evolution(single_ret_raw_value_params, params, local_logdir=True, direction=Direction.MAX)\n",
    "assert_return_values(logdir, hp_dict, True, return_dict, True)\n",
    "assert_best_hyperparameters(hp_dict, {'a': 5, 'b': 2})\n",
    "assert_return_dict(logdir, return_dict)\n",
    "\n",
    "best_logdir, hp_dict, return_dict = experiment.differential_evolution(create_model_in_tensorboard_logdir_params, params, local_logdir=True, name='diff evo model exported from local logdir', optimization_key='optval', direction=Direction.MIN)\n",
    "model.export(best_logdir + '/model', return_dict['name'])\n",
    "\n",
    "best_logdir, hp_dict, return_dict = experiment.differential_evolution(create_model_in_tensorboard_logdir_params, params, local_logdir=False, name='diff evo model exported from hdfs logdir', optimization_key='optval', direction=Direction.MAX)\n",
    "model.export(best_logdir + '/model', return_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "def dist_exp_asserts():\n",
    "    from hops import tensorboard\n",
    "    from hops import devices\n",
    "    from hops import hdfs\n",
    "    import os\n",
    "    assert devices.get_num_gpus()==0\n",
    "    assert hdfs.project_path() == hdfs.project_path(hdfs.project_name())\n",
    "    \n",
    "    tf_config = json.loads(os.environ['TF_CONFIG'])\n",
    "    \n",
    "    role = tf_config['task']['type']\n",
    "    \n",
    "    print(tensorboard.logdir())\n",
    "    \n",
    "    # Only chief and evaluator role should have access to TB logdir to write checkpoints/summary/evaluation etc\n",
    "    if role == 'chief':\n",
    "        assert tensorboard.logdir() != None, 'chief TB is None'\n",
    "        if tensorboard.local_logdir_bool:\n",
    "            assert \"hdfs://\" not in tensorboard.logdir(), 'chief TB is not local'\n",
    "            assert os.path.exists(tensorboard.logdir()), 'chief local TB path does not exists'\n",
    "        else:\n",
    "            assert \"hdfs://\" in tensorboard.logdir(), 'chief TB is not in HDFS'\n",
    "            assert hdfs.exists(tensorboard.logdir()), 'chief hdfs TB path does not exists'\n",
    "    elif role == 'worker' or role == 'ps':\n",
    "        assert tensorboard.logdir() == None, 'ps or worker TB is not None {}'.format(tf_config)\n",
    "    elif role == 'evaluator':\n",
    "        assert tensorboard.logdir() != None, 'evaluator TB is None'\n",
    "        assert hdfs.exists(tensorboard.logdir()), 'evaluator TB path does not exists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Distributed Training `experiment.collective_all_reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "def no_ret():\n",
    "    dist_exp_asserts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "def multi_return():\n",
    "    dist_exp_asserts()\n",
    "    f = open('testfile.txt', 'w')\n",
    "    f.write('stuff happened')\n",
    "    f.close()\n",
    "    f = open('img.png', 'w')\n",
    "    f.write('stuff happened')\n",
    "    f.close()\n",
    "    return {'value': 10, 'morevals': 3, 'logfile': 'testfile.txt', 'diagram': 'img.png'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "experiment.collective_all_reduce(no_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "experiment.collective_all_reduce(multi_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "experiment.collective_all_reduce(export_model_in_wrapper, name='collective model exported in wrapper', local_logdir=False)\n",
    "\n",
    "experiment.collective_all_reduce(export_model_in_wrapper, name='collective model exported in wrapper', local_logdir=True)\n",
    "\n",
    "logdir, return_dict = experiment.collective_all_reduce(create_model_in_tensorboard_logdir, local_logdir=True, name='collective model exported from local logdir')\n",
    "model.export(logdir + '/model', return_dict['name'])\n",
    "\n",
    "logdir, return_dict = experiment.collective_all_reduce(create_model_in_tensorboard_logdir, local_logdir=False, name='collective model exported from hdfs logdir')\n",
    "model.export(logdir + '/model', return_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "experiment.mirrored(no_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "experiment.mirrored(multi_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "experiment.mirrored(export_model_in_wrapper, name='mirrored model exported in wrapper', local_logdir=False)\n",
    "\n",
    "experiment.mirrored(export_model_in_wrapper, name='mirrored model exported in wrapper', local_logdir=True)\n",
    "\n",
    "logdir, return_dict = experiment.mirrored(create_model_in_tensorboard_logdir, local_logdir=True, name='mirrored model exported from local logdir')\n",
    "model.export(logdir + '/model', return_dict['name'])\n",
    "\n",
    "logdir, return_dict = experiment.mirrored(create_model_in_tensorboard_logdir, local_logdir=False, name='mirrored model exported from hdfs logdir')\n",
    "model.export(logdir + '/model', return_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "#experiment.parameter_server(no_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "#experiment.parameter_server(multi_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "#experiment.parameter_server(export_model_in_wrapper, name='mirrored model exported in wrapper', local_logdir=False)\n",
    "\n",
    "#experiment.parameter_server(export_model_in_wrapper, name='mirrored model exported in wrapper', local_logdir=True)\n",
    "\n",
    "#logdir, return_dict = experiment.parameter_server(create_model_in_tensorboard_logdir, local_logdir=True, name='mirrored model exported from local logdir')\n",
    "#serving.export(logdir + '/model', return_dict['name'], 1)\n",
    "\n",
    "#logdir, return_dict = experiment.parameter_server(create_model_in_tensorboard_logdir, local_logdir=False, name='mirrored model exported from hdfs logdir')\n",
    "#serving.export(logdir + '/model', return_dict['name'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HopsFS Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test HopsFS operations\n",
    "\n",
    "- `hdfs.project_user()`\n",
    "- `hdfs.project_name()`\n",
    "- `hdfs.project_path()`\n",
    "- `hdfs.exists()`\n",
    "- `hdfs.load()`\n",
    "- `hdfs.copy_to_hdfs()`\n",
    "- `hdfs.copy_to_local()`\n",
    "- `hdfs.ls()`\n",
    "- `hdfs.lsl()`\n",
    "- `hdfs.glob()`\n",
    "- `hdfs.cp()`\n",
    "- `hdfs.rmr()`\n",
    "- `hdfs.rename()`\n",
    "- `hdfs.stat()`\n",
    "- `hdfs.isdir()`\n",
    "- `hdfs.isfile()`\n",
    "- `hdfs.add_module()`\n",
    "- `hdfs.delete()`\n",
    "- `hdfs.get_plain_path()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "project_user = hdfs.project_user()\n",
    "project_name = hdfs.project_name()\n",
    "assert project_name in project_user\n",
    "project_path = hdfs.project_path()\n",
    "assert project_name in project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "logs_README = hdfs.load(\"Logs/README.md\")\n",
    "assert len(logs_README) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.dump(\"test\", \"Logs/README_dump_test.md\")\n",
    "assert hdfs.exists(\"Logs/README_dump_test.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "logs_README_dumped = hdfs.load(\"Logs/README_dump_test.md\")\n",
    "assert logs_README_dumped.decode(\"utf-8\") == \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# copy_to_hdfs file relative path\n",
    "\n",
    "with open('upload.txt', 'w') as f:\n",
    "    f.write(\"first upload\")\n",
    "hdfs.copy_to_hdfs(\"upload.txt\", \"Resources\")\n",
    "assert hdfs.exists(\"Resources/upload.txt\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload.txt\")\n",
    "assert \"first upload\" == hdfs_copied_file.decode(\"utf-8\"), \"first content does not match\"\n",
    "\n",
    "with open('upload.txt', 'w') as f:\n",
    "    f.write(\"second upload\")\n",
    "hdfs.copy_to_hdfs(\"upload.txt\", \"Resources\", overwrite=True)\n",
    "assert hdfs.exists(\"Resources/upload.txt\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload.txt\")\n",
    "assert \"second upload\" == hdfs_copied_file.decode(\"utf-8\"), \"second content does not match\"\n",
    "\n",
    "try:\n",
    "    hdfs.copy_to_hdfs(\"upload.txt\", \"Resources\")\n",
    "    assert False\n",
    "except IOError:\n",
    "    pass\n",
    "\n",
    "hdfs.rmr(\"Resources/upload.txt\")\n",
    "os.remove(\"upload.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# copy_to_hdfs file absolute path\n",
    "\n",
    "with open('upload_absolute.txt', 'w') as f:\n",
    "    f.write(\"first upload\")\n",
    "hdfs.copy_to_hdfs(os.getcwd() + \"/upload_absolute.txt\", \"Resources\")\n",
    "assert hdfs.exists(\"Resources/upload_absolute.txt\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload_absolute.txt\")\n",
    "assert \"first upload\" == hdfs_copied_file.decode(\"utf-8\"), \"first content does not match\"\n",
    "\n",
    "with open('upload_absolute.txt', 'w') as f:\n",
    "    f.write(\"second upload\")\n",
    "hdfs.copy_to_hdfs(os.getcwd() + \"/upload_absolute.txt\", \"Resources\", overwrite=True)\n",
    "assert hdfs.exists(\"Resources/upload_absolute.txt\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload_absolute.txt\")\n",
    "assert \"second upload\" == hdfs_copied_file.decode(\"utf-8\"), \"second content does not match\"\n",
    "\n",
    "try:\n",
    "    hdfs.copy_to_hdfs(\"upload_absolute.txt\", \"Resources\")\n",
    "    assert False\n",
    "except IOError:\n",
    "    pass\n",
    "\n",
    "hdfs.rmr(\"Resources/upload_absolute.txt\")\n",
    "os.remove(\"upload_absolute.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# copy_to_hdfs directory relative path\n",
    "\n",
    "if not os.path.exists(\"upload_dir\"):\n",
    "    os.mkdir(\"upload_dir\")\n",
    "\n",
    "assert not hdfs.exists(\"Resources/upload_dir\")\n",
    "with open('upload_dir/upload.txt', 'w') as f:\n",
    "    f.write(\"first upload\")\n",
    "hdfs.copy_to_hdfs(\"upload_dir\", \"Resources\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload_dir/upload.txt\")\n",
    "assert hdfs.exists(\"Resources/upload_dir\")\n",
    "with open('upload_dir/upload.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"first content compare failed\"\n",
    "\n",
    "with open('upload_dir/upload.txt', 'w') as f:\n",
    "    f.write(\"second upload\")\n",
    "hdfs.copy_to_hdfs(\"upload_dir\", \"Resources\", overwrite=True)\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload_dir/upload.txt\")\n",
    "assert hdfs.exists(\"Resources/upload_dir\")\n",
    "with open('upload_dir/upload.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"second content compare failed\"\n",
    "\n",
    "shutil.rmtree(\"upload_dir\")\n",
    "hdfs.rmr(\"Resources/upload_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# copy_to_hdfs directory absolute path\n",
    "\n",
    "if not os.path.exists(\"upload_dir_absolute\"):\n",
    "    os.mkdir(\"upload_dir_absolute\")\n",
    "    \n",
    "assert not hdfs.exists(\"Resources/upload_dir_absolute\")\n",
    "with open('upload_dir_absolute/upload.txt', 'w') as f:\n",
    "    f.write(\"first upload\")\n",
    "hdfs.copy_to_hdfs(os.getcwd() + \"/upload_dir_absolute\", \"Resources\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload_dir_absolute/upload.txt\")\n",
    "assert hdfs.exists(\"Resources/upload_dir_absolute\")\n",
    "with open('upload_dir_absolute/upload.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"first content compare failed\"\n",
    "\n",
    "with open('upload_dir_absolute/upload.txt', 'w') as f:\n",
    "    f.write(\"second upload\")\n",
    "hdfs.copy_to_hdfs(os.getcwd() + \"/upload_dir_absolute\", \"Resources\", overwrite=True)\n",
    "hdfs_copied_file = hdfs.load(\"Resources/upload_dir_absolute/upload.txt\")\n",
    "assert hdfs.exists(\"Resources/upload_dir_absolute\")\n",
    "with open('upload_dir_absolute/upload.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"second content compare failed\"\n",
    "\n",
    "shutil.rmtree(\"upload_dir_absolute\")\n",
    "hdfs.rmr(\"Resources/upload_dir_absolute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "#copy_to_local file\n",
    "\n",
    "# Download first time\n",
    "hdfs.dump(\"initial content\", \"Resources/somefile.txt\")\n",
    "hdfs.copy_to_local(\"Resources/somefile.txt\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/somefile.txt\")\n",
    "with open('somefile.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"first content compare failed\"\n",
    "first_modified = os.path.getmtime(\"somefile.txt\")\n",
    "\n",
    "# Download second time\n",
    "hdfs.copy_to_local(\"Resources/somefile.txt\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/somefile.txt\")\n",
    "with open('somefile.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"second content compare failed\"\n",
    "second_modified = os.path.getmtime(\"somefile.txt\")\n",
    "assert first_modified == second_modified, \"modified time not matching\"\n",
    "\n",
    "# Content changing on disk\n",
    "hdfs.dump(\"content changed at some point\", \"Resources/somefile.txt\")\n",
    "hdfs_new_content = hdfs.load(\"Resources/somefile.txt\")\n",
    "hdfs.copy_to_local(\"Resources/somefile.txt\")\n",
    "with open('somefile.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_new_content.decode(\"utf-8\") == local_copied_file, \"third content compare failed\"\n",
    "third_modified = os.path.getmtime(\"somefile.txt\")\n",
    "assert not second_modified == third_modified, \"modified time not matching\"\n",
    "\n",
    "# Download last time with overwrite, file should have changed on disk\n",
    "hdfs.copy_to_local(\"Resources/somefile.txt\", overwrite=True)\n",
    "hdfs_copied_file = hdfs.load(\"Resources/somefile.txt\")\n",
    "with open('somefile.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"fourth content compare failed\"\n",
    "fourth_modified = os.path.getmtime(\"somefile.txt\")\n",
    "assert not third_modified == fourth_modified, \"modified time not matching\"\n",
    "\n",
    "# Download again to make sure overwrite did not cause problems\n",
    "hdfs.copy_to_local(\"Resources/somefile.txt\")\n",
    "hdfs_copied_file = hdfs.load(\"Resources/somefile.txt\")\n",
    "with open('somefile.txt', 'r') as f:\n",
    "    local_copied_file = f.read()\n",
    "assert hdfs_copied_file.decode(\"utf-8\") == local_copied_file, \"fifth content compare failed\"\n",
    "fifth_modified = os.path.getmtime(\"somefile.txt\")\n",
    "assert fourth_modified == fifth_modified, \"modified time not matching\"\n",
    "\n",
    "hdfs.rmr(\"Resources/somefile.txt\")\n",
    "os.remove(\"somefile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "#copy_to_local directory\n",
    "\n",
    "assert not os.path.exists(\"Resources\")\n",
    "hdfs.copy_to_local(\"Resources\")\n",
    "first_modified = os.path.getmtime(\"Resources\")\n",
    "assert os.path.exists(\"Resources\")\n",
    "assert os.path.isdir(\"Resources\")\n",
    "\n",
    "hdfs.copy_to_local(\"Resources\")\n",
    "second_modified = os.path.getmtime(\"Resources\")\n",
    "assert first_modified == second_modified\n",
    "\n",
    "localized_dir = hdfs.copy_to_local(\"Resources\", overwrite=True)\n",
    "third_modified = os.path.getmtime(\"Resources\")\n",
    "assert not second_modified == third_modified\n",
    "num_files_first = len(os.listdir(localized_dir))\n",
    "\n",
    "# Add a new file, it should also be localized\n",
    "hdfs.dump(\"a wild file appeared\", \"Resources/newfile.txt\")\n",
    "hdfs.copy_to_local(\"Resources\")\n",
    "fourth_modified = os.path.getmtime(\"Resources\")\n",
    "assert first_modified == second_modified\n",
    "num_files_second = len(os.listdir(localized_dir))\n",
    "assert (num_files_first + 1) == num_files_second\n",
    "assert not third_modified == fourth_modified\n",
    "\n",
    "hdfs.rmr(\"Resources/newfile.txt\")\n",
    "shutil.rmtree(\"Resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "logs_files_md = hdfs.glob(\"Logs/*.md\")\n",
    "logs_path_names = hdfs.lsl(\"Logs/\")\n",
    "if hdfs.exists(\"Logs/test.txt\"):\n",
    "    hdfs.rmr(\"Logs/test.txt\")\n",
    "assert not hdfs.exists(\"Logs/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.dump(\"dummy\", \"Resources/test.txt\")\n",
    "hdfs.cp(\"Resources/test.txt\", \"Logs/\")\n",
    "logs_files = hdfs.ls(\"Logs/\")\n",
    "assert \"test.txt\" in \",\".join(logs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.mkdir(\"Logs/test_dir\")\n",
    "assert hdfs.exists(\"Logs/test_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "logs_files_prior_delete = hdfs.ls(\"Logs/\")\n",
    "hdfs.rmr(\"Logs/test_dir\")\n",
    "logs_files_after_delete = hdfs.ls(\"Logs/\")\n",
    "assert len(logs_files_prior_delete) > len(logs_files_after_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "logs_files_prior_move = hdfs.ls(\"Logs/\")\n",
    "assert \"README_dump_test.md\" in \",\".join(logs_files_prior_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.move(\"Logs/README_dump_test.md\", \"Logs/README_dump_test2.md\")\n",
    "logs_files_after_move = hdfs.ls(\"Logs/\")\n",
    "assert \"README_dump_test.md\" not in \",\".join(logs_files_after_move)\n",
    "assert \"README_dump_test2.md\" in \",\".join(logs_files_after_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "logs_files_prior_rename = hdfs.ls(\"Logs/\")\n",
    "assert \"README_dump_test2.md\" in \",\".join(logs_files_prior_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.rename(\"Logs/README_dump_test2.md\", \"Logs/README_dump_test.md\")\n",
    "logs_files_after_rename = hdfs.ls(\"Logs/\")\n",
    "assert \"Logs/README_dump_test2.md\" not in \",\".join(logs_files_after_rename)\n",
    "assert \"Logs/README_dump_test.md\" in \",\".join(logs_files_after_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "file_stat = hdfs.stat(\"Logs/README.md\")\n",
    "hdfs.chmod(\"Logs/README.md\", 775)\n",
    "file_stat = hdfs.stat(\"Logs/README.md\")\n",
    "assert 775 == file_stat.st_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.chmod(\"Logs/README.md\", 777)\n",
    "file_stat = hdfs.stat(\"Logs/README.md\")\n",
    "assert 777 == file_stat.st_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "file_owner = file_stat.st_uid\n",
    "assert hdfs.exists(\"Logs/\")\n",
    "assert not hdfs.exists(\"Not_Existing/neither_am_i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert hdfs.isdir(\"Resources\")\n",
    "assert not hdfs.isdir(\"Resources/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert hdfs.isfile(\"Resources/README.md\")\n",
    "assert not hdfs.isfile(\"Resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.dump(\"def simple():\\n\\treturn 5\", \"Resources/my_module.py\")\n",
    "py_path = hdfs.add_module(\"Resources/my_module.py\")\n",
    "assert py_path in sys.path\n",
    "import my_module\n",
    "assert my_module.simple() == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "plain = hdfs.get_plain_path(\"hdfs://10.0.2.15:8020/Projects/demo_deep_learning_admin000/Models/\")\n",
    "assert plain == \"/Projects/demo_deep_learning_admin000/Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "hdfs.mkdir(\"Logs/test_delete_dir\")\n",
    "assert hdfs.exists(\"Logs/test_delete_dir\")\n",
    "hdfs.delete(\"Logs/test_delete_dir\")\n",
    "assert not hdfs.exists(\"Logs/test_delete_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store Tests\n",
    "\n",
    "These tests require that you have the following files in the Resources directory:\n",
    "\n",
    "- `attendances_features.csv`\n",
    "- `games_features.csv`\n",
    "- `players_features.csv`\n",
    "- `season_scores_features.csv`\n",
    "- `teams_features.csv`\n",
    "\n",
    "These files can be downloaded from here: `http://snurran.sics.se/hops/hops-util-py_test/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Featurestore Create Feature Group Operations (`featurestore.create_featuregroup()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "def load_fs_sample_data():\n",
    "    resources_path = hdfs.project_path() + \"Resources/\"\n",
    "    games_features_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(resources_path + \"games_features.csv\")\n",
    "    players_features_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(resources_path + \"players_features.csv\")\n",
    "    teams_features_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(resources_path + \"teams_features.csv\")\n",
    "    season_scores_features_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(resources_path + \"season_scores_features.csv\")\n",
    "    attendances_features_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(resources_path + \"attendances_features.csv\")\n",
    "    return games_features_df,players_features_df,teams_features_df,season_scores_features_df, attendances_features_df\n",
    "games_features_df,players_features_df,teams_features_df,season_scores_features_df, attendances_features_df = load_fs_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    games_features_df,\n",
    "    \"games_features\",\n",
    "    description=\"Features of average season scores for football teams\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_df,\n",
    "    \"teams_features\",\n",
    "    description=\"a spanish version of teams_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    season_scores_features_df,\n",
    "    \"season_scores_features\",\n",
    "    description=\"Features of average season scores for football teams\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    attendances_features_df,\n",
    "    \"attendances_features\",\n",
    "    description=\"Features of average attendance of games of football teams\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "teams_features_1_df = featurestore.get_featuregroup(\"teams_features\")\n",
    "teams_features_2_df = teams_features_1_df.withColumnRenamed(\n",
    "    \"team_id\", \"equipo_id\").withColumnRenamed(\n",
    "    \"team_budget\", \"equipo_presupuesto\").withColumnRenamed(\n",
    "    \"team_position\", \"equipo_posicion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroup_version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    featuregroup_version=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "query = \"SELECT * FROM games_features_1 WHERE score > 1\"\n",
    "storage_connector = hdfs.project_name() + \"_featurestore\"\n",
    "featuregroup_name = \"games_features_on_demand\"\n",
    "featurestore.create_on_demand_featuregroup(query, featuregroup_name, storage_connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert \"games_features_1\" in featurestore.get_featuregroups()\n",
    "assert \"teams_features_1\" in featurestore.get_featuregroups()\n",
    "assert \"season_scores_features_1\" in featurestore.get_featuregroups()\n",
    "assert \"attendances_features_1\" in featurestore.get_featuregroups()\n",
    "assert \"teams_features_spanish_1\" in featurestore.get_featuregroups()\n",
    "assert \"teams_features_spanish_2\" in featurestore.get_featuregroups()\n",
    "assert \"games_features_on_demand_1\" in featurestore.get_featuregroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Featurestore Utility Operations, \n",
    "\n",
    "- `featurestore.get_metadata()`,\n",
    "- `featurestore.project_featurestore()`, \n",
    "- `featurestore.get_latest_featuregroup_version()`, \n",
    "- `featurestore.get_features_list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.get_featurestore_metadata(update_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert featurestore.project_featurestore() == hdfs.project_name() + \"_featurestore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert featurestore.project_featurestore() in featurestore.get_project_featurestores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert len(featurestore.get_project_featurestores()) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert featurestore.get_latest_featuregroup_version(\"teams_features_spanish\") == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert featurestore.get_latest_featuregroup_version(\"teams_features\") == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert \"away_team_id\" in featurestore.get_features_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert \"home_team_id\" in featurestore.get_features_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert (hdfs.project_name() + \"_featurestore\", 'JDBC') in featurestore.get_storage_connectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert len(featurestore.get_storage_connectors()) >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Read operations of Features and Feature Groups, \n",
    "\n",
    "- `featurestore.get_feature()`, \n",
    "- `featurestore.get_features()`, \n",
    "- `featurestore.get_featuregroup()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_feature(\"team_budget\")\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == 1\n",
    "assert \"team_budget\" in tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_feature(\n",
    "    \"team_budget\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup=\"teams_features\", \n",
    "    featuregroup_version = 1,\n",
    "    dataframe_type = \"spark\"\n",
    ")\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == 1\n",
    "assert \"team_budget\" in tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_featuregroup(\"teams_features\")\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == 3\n",
    "assert \"team_budget\" in tmp.columns\n",
    "assert \"team_id\" in tmp.columns\n",
    "assert \"team_position\" in tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_featuregroup(\n",
    "    \"teams_features\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup_version = 1,\n",
    "    dataframe_type = \"spark\"\n",
    ")\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == 3\n",
    "assert \"team_budget\" in tmp.columns\n",
    "assert \"team_id\" in tmp.columns\n",
    "assert \"team_position\" in tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "features = [\"team_budget\", \"average_attendance\"]\n",
    "tmp = featurestore.get_features(\n",
    "    features\n",
    ")\n",
    "assert set(features) == set(tmp.columns)\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "features = [\"teams_features_1.team_budget\", \"attendances_features_1.average_attendance\"]\n",
    "tmp = featurestore.get_features(features)\n",
    "assert set([\"team_budget\", \"average_attendance\"]) == set(tmp.columns)\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "features = [\"team_budget\", \"average_attendance\"]\n",
    "tmp = featurestore.get_features(\n",
    "    features,\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroups_version_dict={\n",
    "        \"teams_features\": 1, \n",
    "        \"attendances_features\": 1\n",
    "    }\n",
    ")\n",
    "assert set(features) == set(tmp.columns)\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_features(\n",
    "    features,\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroups_version_dict={\n",
    "        \"teams_features\": 1, \n",
    "        \"attendances_features\": 1\n",
    "    },\n",
    "    join_key = \"team_id\",\n",
    "    dataframe_type = \"spark\"\n",
    ")\n",
    "assert set(features) == set(tmp.columns)\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "features = [\"team_budget\", \"average_attendance\",\n",
    "    \"team_position\", \"sum_attendance\"\n",
    "    ]\n",
    "tmp = featurestore.get_features(\n",
    "   features\n",
    ")\n",
    "assert set(features) == set(tmp.columns)\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "features = [\"team_budget\", \"team_id\"]\n",
    "tmp = featurestore.get_features(\n",
    "    features,\n",
    "    featuregroups_version_dict = {\n",
    "        \"teams_features\" : 1\n",
    "    }\n",
    ")\n",
    "assert set(features) == set(tmp.columns)\n",
    "assert tmp.count() == 50\n",
    "assert len(tmp.columns) == len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.sql(\n",
    "    \"SELECT team_budget, score \" \\\n",
    "    \"FROM teams_features_1 JOIN games_features_1 ON \" \\\n",
    "    \"games_features_1.home_team_id = teams_features_1.team_id\")\n",
    "features = ['team_budget', 'score']\n",
    "assert set(features) == set(tmp.columns)\n",
    "assert tmp.count() == 49\n",
    "assert len(tmp.columns) == len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.sql(\"SELECT * FROM teams_features_1 WHERE team_position < 5\")\n",
    "assert len(tmp.columns) == 3\n",
    "assert \"team_budget\" in tmp.columns\n",
    "assert \"team_id\" in tmp.columns\n",
    "assert \"team_position\" in tmp.columns\n",
    "for x in tmp.toPandas()[\"team_position\"].values:\n",
    "    assert x < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.sql(\"SELECT * FROM teams_features_1 WHERE team_position < 5\",\n",
    "                featurestore=featurestore.project_featurestore(), \n",
    "                 dataframe_type = \"spark\")\n",
    "assert len(tmp.columns) == 3\n",
    "assert \"team_budget\" in tmp.columns\n",
    "assert \"team_id\" in tmp.columns\n",
    "assert \"team_position\" in tmp.columns\n",
    "for x in tmp.toPandas()[\"team_position\"].values:\n",
    "    assert x < 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Test Insert Operations in Existing Feature Groups, `featurestore.insert_into_featuregroup()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "schema = StructType([StructField(\"equipo_id\", IntegerType(), True),\n",
    "                     StructField(\"equipo_presupuesto\", FloatType(), True),\n",
    "                     StructField(\"equipo_posicion\", IntegerType(), True)\n",
    "                        ])\n",
    "sample_df = sqlContext.createDataFrame([(999, 41251.52, 1), (998, 1319.4, 8), (997, 21219.1, 2)], schema)\n",
    "insert_count = sample_df.count()\n",
    "assert insert_count == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "spanish_team_features_df = featurestore.get_featuregroup(\n",
    "    \"teams_features_spanish\")\n",
    "pre_insert_count = spanish_team_features_df.count()\n",
    "assert pre_insert_count == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\", \n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")\n",
    "spanish_team_features_df_updated = featurestore.get_featuregroup(\n",
    "    \"teams_features_spanish\")\n",
    "\n",
    "after_insert_count = spanish_team_features_df_updated.count()\n",
    "assert after_insert_count == 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup_version=1, \n",
    "    mode=\"append\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False, \n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False, \n",
    "    stat_columns=None, \n",
    "    num_bins=20, \n",
    "    corr_method='pearson',\n",
    "    num_clusters=5\n",
    ")\n",
    "\n",
    "after_insert_count2 = featurestore.get_featuregroup(\"teams_features_spanish\").count()\n",
    "assert after_insert_count2 == 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")\n",
    "\n",
    "count_after_overwrite = featurestore.get_featuregroup(\"teams_features_spanish\").count()\n",
    "assert count_after_overwrite == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test integration of feature store with Numpy, Pandas and plain Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "pandas_df = featurestore.get_features([\"team_budget\", \"average_attendance\"], dataframe_type=\"pandas\")\n",
    "assert \"team_budget\" in pandas_df.columns.values\n",
    "assert \"average_attendance\" in pandas_df.columns.values\n",
    "assert len(pandas_df) == 50\n",
    "assert len(pandas_df.columns.values) == 2\n",
    "assert isinstance(pandas_df, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "numpy_df = featurestore.get_features([\"team_budget\", \"average_attendance\"], \n",
    "                                      dataframe_type=\"numpy\")\n",
    "assert numpy_df.shape[0] == 50\n",
    "assert numpy_df.shape[1] == 2\n",
    "assert isinstance(numpy_df, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "python_df = featurestore.get_features([\"team_budget\", \"average_attendance\"], \n",
    "                                      dataframe_type=\"python\")\n",
    "assert len(python_df) == 50\n",
    "assert isinstance(python_df, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "spark_df = featurestore.get_features([\"team_budget\", \"average_attendance\"], \n",
    "                                      dataframe_type=\"spark\")\n",
    "assert spark_df.count() == 50\n",
    "assert isinstance(spark_df, DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# Let's rename the columns to differentiate this feature group from existing ones in the feature store\n",
    "pandas_df.columns = [\"team_budget_test\", \"average_attendance_test\"]\n",
    "\n",
    "featurestore.create_featuregroup(\n",
    "    pandas_df,\n",
    "    \"pandas_test_example\",\n",
    "    description=\"test featuregroup created from pandas dataframe\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")\n",
    "assert \"pandas_test_example_1\" in featurestore.get_featuregroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "count_pre_pandas_insert_overwrite = featurestore.get_featuregroup(\"pandas_test_example\").count()\n",
    "featurestore.insert_into_featuregroup(\n",
    "    pandas_df, \n",
    "    \"pandas_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")\n",
    "count_after_pandas_insert_overwrite = featurestore.get_featuregroup(\"pandas_test_example\").count()\n",
    "assert count_pre_pandas_insert_overwrite == count_after_pandas_insert_overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    numpy_df,\n",
    "    \"numpy_test_example\",\n",
    "    description=\"test featuregroup created from numpy matrix\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")\n",
    "assert \"numpy_test_example_1\" in featurestore.get_featuregroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "numpy_test_df_count_pre_insert_overwrite = featurestore.get_featuregroup(\"numpy_test_example\", dataframe_type=\"spark\").count()\n",
    "featurestore.insert_into_featuregroup(\n",
    "    numpy_df, \n",
    "    \"numpy_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")\n",
    "numpy_test_df_count_after_insert_overwrite = featurestore.get_featuregroup(\"numpy_test_example\", dataframe_type=\"spark\").count()\n",
    "assert numpy_test_df_count_pre_insert_overwrite == numpy_test_df_count_pre_insert_overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    python_df,\n",
    "    \"python_test_example\",\n",
    "    description=\"test featuregroup created from python 2D list\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")\n",
    "\n",
    "python_test_df_count_pre_insert_overwrite = featurestore.get_featuregroup(\"python_test_example\", dataframe_type=\"spark\").count()\n",
    "assert \"python_test_example_1\" in featurestore.get_featuregroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    python_df, \n",
    "    \"python_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")\n",
    "\n",
    "python_test_df_count_after_insert_overwrite = featurestore.get_featuregroup(\"python_test_example\", dataframe_type=\"spark\").count()\n",
    "assert python_test_df_count_pre_insert_overwrite == python_test_df_count_after_insert_overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test update Feature Store Statistics `featurestore.update_featuregroup_stats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.update_featuregroup_stats(\"teams_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.update_featuregroup_stats(\n",
    "    \"teams_features\", \n",
    "    featuregroup_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Write Training Dataset Operations \n",
    "\n",
    "- `featurestore.get_latest_training_dataset_version()`\n",
    "- `create_training_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "features_df = featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\",\n",
    "    \"team_position\"]\n",
    ")\n",
    "latest_version = featurestore.get_latest_training_dataset_version(\"team_position_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    training_dataset_version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_csv\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"csv\",\n",
    "    training_dataset_version= 1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_tsv\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"tsv\",\n",
    "    training_dataset_version=1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_parquet\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"parquet\",\n",
    "    training_dataset_version=1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_orc\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"orc\",\n",
    "    training_dataset_version=1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_avro\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"avro\",\n",
    "    training_dataset_version=1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_hdf5\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"hdf5\",\n",
    "    training_dataset_version=1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_npy\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"npy\",\n",
    "    training_dataset_version=1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# Petastorm is only supported in python 3\n",
    "if sys.version_info[0] >= 3:\n",
    "    PetastormSchema = Unischema('team_position_prediction_petastorm_schema', [\n",
    "        UnischemaField('team_budget', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "        UnischemaField('average_attendance', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "        UnischemaField('team_position', np.int32, (), ScalarCodec(IntegerType()), False)\n",
    "    ])\n",
    "\n",
    "    petastorm_args = {\n",
    "        \"schema\": PetastormSchema\n",
    "    }\n",
    "\n",
    "    featurestore.create_training_dataset(\n",
    "        features_df, \"team_position_prediction_petastorm\",\n",
    "        description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "        featurestore=featurestore.project_featurestore(),\n",
    "        data_format=\"petastorm\",\n",
    "        training_dataset_version=1,\n",
    "        descriptive_statistics=False,\n",
    "        feature_correlation=False,\n",
    "        feature_histograms=False,\n",
    "        cluster_analysis=False,\n",
    "        stat_columns=None,\n",
    "        petastorm_args=petastorm_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tds = featurestore.get_training_datasets()\n",
    "assert 'team_position_prediction_1' in tds\n",
    "assert 'team_position_prediction_csv_1' in tds\n",
    "assert 'team_position_prediction_tsv_1' in tds\n",
    "assert 'team_position_prediction_parquet_1' in tds\n",
    "assert 'team_position_prediction_orc_1' in tds\n",
    "assert 'team_position_prediction_avro_1' in tds\n",
    "assert 'team_position_prediction_hdf5_1'in tds\n",
    "assert 'team_position_prediction_npy_1' in tds\n",
    "if sys.version_info[0] >= 3:\n",
    "    assert 'team_position_prediction_petastorm_1' in tds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Insert into an existing training dataset, `featurestore.insert_into_training_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "count_pre_insert = featurestore.get_training_dataset(\"team_position_prediction_csv\").count()\n",
    "featurestore.insert_into_training_dataset(\n",
    "    features_df, \n",
    "    \"team_position_prediction_csv\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\")\n",
    ")\n",
    "count_after_insert = featurestore.get_training_dataset(\"team_position_prediction_csv\").count()\n",
    "assert count_pre_insert == count_after_insert # td only support overwrites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Training Dataset Utility Methods\n",
    "\n",
    "- `featurestore.get_training_dataset_path()`\n",
    "- `featurestore.get_training_dataset_tf_record_schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert hdfs.project_path() in featurestore.get_training_dataset_path(\"team_position_prediction_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert hdfs.project_name() + \"_Training_Datasets\" in featurestore.get_training_dataset_path(\"team_position_prediction_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert \"team_position_prediction_csv\" in featurestore.get_training_dataset_path(\"team_position_prediction_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tf_schema = featurestore.get_training_dataset_tf_record_schema(\"team_position_prediction\")\n",
    "assert tf_schema == {'team_budget': tf.FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), \n",
    "                     'average_attendance': tf.FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), \n",
    "                     'team_position': tf.FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "features_df = featurestore.get_training_dataset(\"team_position_prediction\")\n",
    "tf_schema = featurestore.get_dataframe_tf_record_schema(features_df)\n",
    "assert tf_schema == {'team_budget': tf.FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), \n",
    "                     'average_attendance': tf.FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), \n",
    "                     'team_position': tf.FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test update Training Dataset stats\n",
    "\n",
    "- `featurestore.update_training_dataset_stats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.update_training_dataset_stats(\"team_position_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.update_training_dataset_stats(\n",
    "    \"team_position_prediction\", \n",
    "    training_dataset_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Read Training Datasets API `featurestore.get_training_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "cols = ['team_budget', 'average_attendance', 'team_position']\n",
    "tmp = featurestore.get_training_dataset(\"team_position_prediction_csv\")\n",
    "assert set(tmp.columns) == set(cols)\n",
    "assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_training_dataset(\"team_position_prediction_hdf5\")\n",
    "assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "if sys.version_info[0] >= 3:\n",
    "    tmp = featurestore.get_training_dataset(\"team_position_prediction_petastorm\")\n",
    "    assert set(tmp.columns) == set(cols)\n",
    "    assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_training_dataset(\"team_position_prediction_avro\")\n",
    "assert set(tmp.columns) == set(cols)\n",
    "assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_training_dataset(\"team_position_prediction_orc\")\n",
    "assert set(tmp.columns) == set(cols)\n",
    "assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_training_dataset(\"team_position_prediction_tsv\")\n",
    "assert set(tmp.columns) == set(cols)\n",
    "assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_training_dataset(\"team_position_prediction_npy\")\n",
    "assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "tmp = featurestore.get_training_dataset(\"team_position_prediction_parquet\")\n",
    "assert set(tmp.columns) == set(cols)\n",
    "assert tmp.count() == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Featurestore Get Statistics\n",
    "\n",
    "- `featurestore.get_featuregroup_statistics()`\n",
    "- `featurestore.get_training_dataset_statistics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "stats = featurestore.get_featuregroup_statistics(\"teams_features\")\n",
    "assert not stats.cluster_analysis is None\n",
    "assert not stats.cluster_analysis.clusters is None\n",
    "assert not stats.cluster_analysis.datapoints is None\n",
    "assert len(stats.cluster_analysis.clusters) == len(stats.cluster_analysis.datapoints)\n",
    "assert not stats.cluster_analysis.clusters[0].datapoint_name is None\n",
    "assert not stats.cluster_analysis.clusters[0].cluster is None\n",
    "assert not stats.correlation_matrix is None\n",
    "assert not stats.correlation_matrix.feature_correlations is None\n",
    "assert len(stats.correlation_matrix.feature_correlations) > 0\n",
    "assert len(stats.correlation_matrix.feature_correlations) < constants.FEATURE_STORE.MAX_CORRELATION_MATRIX_COLUMNS\n",
    "assert not stats.correlation_matrix.feature_correlations[0].feature_name is None\n",
    "assert not stats.correlation_matrix.feature_correlations[0].correlation_values is None\n",
    "assert len(stats.correlation_matrix.feature_correlations[0].correlation_values) == \\\n",
    "len(stats.correlation_matrix.feature_correlations)\n",
    "assert not stats.descriptive_stats is None\n",
    "assert not stats.descriptive_stats.descriptive_stats is None\n",
    "assert len(stats.descriptive_stats.descriptive_stats) > 0\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].feature_name is None\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].metric_values is None\n",
    "assert len(stats.descriptive_stats.descriptive_stats[0].metric_values) > 0\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].metric_values[0].metric_name is None\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].metric_values[0].value is None\n",
    "assert not stats.feature_histograms is None\n",
    "assert not stats.feature_histograms.feature_distributions is None\n",
    "assert len(stats.feature_histograms.feature_distributions) > 0\n",
    "assert not stats.feature_histograms.feature_distributions[0].feature_name is None\n",
    "assert not stats.feature_histograms.feature_distributions[0].frequency_distribution is None\n",
    "assert len(stats.feature_histograms.feature_distributions[0].frequency_distribution) > 0\n",
    "assert not stats.feature_histograms.feature_distributions[0].frequency_distribution[0].bin is None\n",
    "assert not stats.feature_histograms.feature_distributions[0].frequency_distribution[0].frequency is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "stats = featurestore.get_training_dataset_statistics(\"team_position_prediction\")\n",
    "assert not stats.cluster_analysis is None\n",
    "assert not stats.cluster_analysis.clusters is None\n",
    "assert not stats.cluster_analysis.datapoints is None\n",
    "assert len(stats.cluster_analysis.clusters) == len(stats.cluster_analysis.datapoints)\n",
    "assert not stats.cluster_analysis.clusters[0].datapoint_name is None\n",
    "assert not stats.cluster_analysis.clusters[0].cluster is None\n",
    "assert not stats.correlation_matrix is None\n",
    "assert not stats.correlation_matrix.feature_correlations is None\n",
    "assert len(stats.correlation_matrix.feature_correlations) > 0\n",
    "assert len(stats.correlation_matrix.feature_correlations) < constants.FEATURE_STORE.MAX_CORRELATION_MATRIX_COLUMNS\n",
    "assert not stats.correlation_matrix.feature_correlations[0].feature_name is None\n",
    "assert not stats.correlation_matrix.feature_correlations[0].correlation_values is None\n",
    "assert len(stats.correlation_matrix.feature_correlations[0].correlation_values) == len(stats.correlation_matrix.feature_correlations)\n",
    "assert not stats.descriptive_stats is None\n",
    "assert not stats.descriptive_stats.descriptive_stats is None\n",
    "assert len(stats.descriptive_stats.descriptive_stats) > 0\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].feature_name is None\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].metric_values is None\n",
    "assert len(stats.descriptive_stats.descriptive_stats[0].metric_values) > 0\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].metric_values[0].metric_name is None\n",
    "assert not stats.descriptive_stats.descriptive_stats[0].metric_values[0].value is None\n",
    "assert not stats.feature_histograms is None\n",
    "assert not stats.feature_histograms.feature_distributions is None\n",
    "assert len(stats.feature_histograms.feature_distributions) > 0\n",
    "assert not stats.feature_histograms.feature_distributions[0].feature_name is None\n",
    "assert not stats.feature_histograms.feature_distributions[0].frequency_distribution is None\n",
    "assert len(stats.feature_histograms.feature_distributions[0].frequency_distribution) > 0\n",
    "assert not stats.feature_histograms.feature_distributions[0].frequency_distribution[0].bin is None\n",
    "assert not stats.feature_histograms.feature_distributions[0].frequency_distribution[0].frequency is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Featurestore Visualizations\n",
    "\n",
    "- `featurestore.visualize_featuregroup_distributions()`\n",
    "- `featurestore.visualize_featuregroup_correlations()`\n",
    "- `featurestore.visualize_featuregroup_clusters()`\n",
    "- `featurestore.visualize_featuregroup_descriptive_stats()`\n",
    "- `featurestore.visualize_training_dataset_distributions()`\n",
    "- `featurestore.visualize_training_dataset_correlations()`\n",
    "- `featurestore.visualize_traniing_dataset_clusters()`\n",
    "- `featurestore.visualize_training_dataset_descriptive_stats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "fig = featurestore.visualize_featuregroup_distributions(\"teams_features\", plot=False)\n",
    "fig.savefig(\"teams_features_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "fig = featurestore.visualize_featuregroup_correlations(\"teams_features\", plot=False)\n",
    "fig.savefig(\"teams_features_correlations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "fig = featurestore.visualize_featuregroup_clusters(\"teams_features\", plot=False)\n",
    "fig.savefig(\"teams_features_clusters.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "desc_stats_df = featurestore.visualize_featuregroup_descriptive_stats(\"teams_features\")\n",
    "desc_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "fig = featurestore.visualize_training_dataset_distributions(\"team_position_prediction\", plot=False)\n",
    "fig.savefig(\"team_position_prediction_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "fig = featurestore.visualize_training_dataset_correlations(\"team_position_prediction\", plot=False)\n",
    "fig.savefig(\"team_position_prediction_correlations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "fig = featurestore.visualize_training_dataset_clusters(\"team_position_prediction\", plot=False)\n",
    "fig.savefig(\"team_position_prediction_clusters.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "desc_stats_df = featurestore.visualize_training_dataset_descriptive_stats(\"team_position_prediction\")\n",
    "desc_stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleanup (Delete FS Contents so that next test run works the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# Delete feature groups\n",
    "spark.sql('use ' + featurestore.project_featurestore())\n",
    "for fg in featurestore.get_featuregroups():\n",
    "    try:\n",
    "        spark.sql(\"drop table \" + fg)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# Delete training datasets\n",
    "td_dir = hdfs.project_name() + \"_Training_Datasets/\"\n",
    "for td in featurestore.get_training_datasets():\n",
    "    try:\n",
    "        hdfs.rmr(td_dir + td)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "featurestore.get_featurestore_metadata(update_cache=True)\n",
    "# on demand feature group will still be there.. maybe add delete endpoint in the python SDK?\n",
    "#assert featurestore.get_featuregroups() == [] \n",
    "assert featurestore.get_training_datasets() == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test default config \n",
    "\n",
    "- `kafka.get_default_config()`, \n",
    "- `kafka.get_security_protocol()`,\n",
    "- `kafka.get_broker_endpoints_list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "config = kafka.get_kafka_default_config()\n",
    "assert \"bootstrap.servers\" in config\n",
    "assert \"security.protocol\" in config\n",
    "assert \"ssl.ca.location\" in config\n",
    "assert \"ssl.key.location\" in config\n",
    "assert \"ssl.certificate.location\" in config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert len(kafka.get_security_protocol()) > 0\n",
    "assert len(kafka.get_broker_endpoints_list()) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLS Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test access to TLS tokens\n",
    "\n",
    "- `tls.get_key_store()`\n",
    "- `tls.get_trust_store()`\n",
    "- `tls.get_key_store_pwd()`\n",
    "- `tls.get_trust_store_pwd()`\n",
    "- `tls.get_client_certificate_location()`\n",
    "- `tls.get_client_key_location()`\n",
    "- `tls.get_ca_chain_location()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://10.0.2.15:8998/sessions/16 with error payload: {\"msg\":\"Session '16' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "assert len(tls.get_key_store()) > 0\n",
    "assert len(tls.get_trust_store()) > 0\n",
    "assert len(tls.get_key_store_pwd()) > 0\n",
    "assert len(tls.get_trust_store_pwd()) > 0\n",
    "assert len(tls.get_client_certificate_location()) > 0\n",
    "assert len(tls.get_client_key_location()) > 0\n",
    "assert len(tls.get_ca_chain_location()) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving Tests\n",
    "\n",
    "These tests require that you have the following files in the Resources directory:\n",
    "\n",
    "- `iris_model.knn`\n",
    "- `iris_flower_classifier.py`\n",
    "- `mnist`\n",
    "\n",
    "Where mnist is a directory containing a tensorflow model.\n",
    "\n",
    "These files can be downloaded from here: `http://snurran.sics.se/hops/hops-util-py_test/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Export Model HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model IrisFlowerClassifier as version 1 successfully.\n",
      "Polling IrisFlowerClassifier version 1 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier as version 2 successfully.\n",
      "Polling IrisFlowerClassifier version 2 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier as version 3 successfully.\n",
      "Polling IrisFlowerClassifier version 3 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier as version 4 successfully.\n",
      "Polling IrisFlowerClassifier version 4 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier as version 5 successfully.\n",
      "Polling IrisFlowerClassifier version 5 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier_abs as version 11 successfully.\n",
      "Polling IrisFlowerClassifier_abs version 11 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier_abs as version 12 successfully.\n",
      "Polling IrisFlowerClassifier_abs version 12 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier_abs as version 13 successfully.\n",
      "Polling IrisFlowerClassifier_abs version 13 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier_abs as version 14 successfully.\n",
      "Polling IrisFlowerClassifier_abs version 14 for model availability.\n",
      "Model now available.\n",
      "Exported model IrisFlowerClassifier_abs as version 15 successfully.\n",
      "Polling IrisFlowerClassifier_abs version 15 for model availability.\n",
      "Model now available."
     ]
    }
   ],
   "source": [
    "model_path_relative = \"Resources\"\n",
    "\n",
    "model.export(model_path_relative, \"IrisFlowerClassifier\", model_version=1, overwrite=True)\n",
    "model.export(model_path_relative, \"IrisFlowerClassifier\", model_version=2, overwrite=True, metrics={'accuracy': 21.5, 'loss': 31.3})\n",
    "model.export(model_path_relative, \"IrisFlowerClassifier\", model_version=3, overwrite=True, metrics={'accuracy': 0.5})\n",
    "model.export(model_path_relative, \"IrisFlowerClassifier\", model_version=4, overwrite=True, metrics={'accuracy': 9})\n",
    "model.export(model_path_relative, \"IrisFlowerClassifier\", model_version=5, overwrite=True, metrics={'accuracy': 30})\n",
    "\n",
    "model_path_abs = hdfs.project_path() + \"Resources\"\n",
    "\n",
    "model.export(model_path_abs, \"IrisFlowerClassifier_abs\", metrics={'accuracy': 10.9})\n",
    "model.export(model_path_abs, \"IrisFlowerClassifier_abs\", metrics={'accuracy': 21.5, 'loss': 31.3})\n",
    "model.export(model_path_abs, \"IrisFlowerClassifier_abs\", metrics={'accuracy': 0.5})\n",
    "model.export(model_path_abs, \"IrisFlowerClassifier_abs\")\n",
    "model.export(model_path_abs, \"IrisFlowerClassifier_abs\", metrics={'accuracy': 30})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Export Model Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started copying local path /srv/hops/hopsdata/tmp/nm-local-dir/usercache/NLucCipWZt56uigje1cP3j3_fYyvpEcQ7n_WV6vHxY8/appcache/application_1571991823790_0002/container_e03_1571991823790_0002_01_000001/model/model.pb to hdfs path hdfs://10.0.2.15:8020/Projects/fawfawdasd/Models/local_model_dir/11\n",
      "\n",
      "Finished copying\n",
      "\n",
      "Exported model local_model_dir as version 11 successfully.\n",
      "Polling local_model_dir version 11 for model availability.\n",
      "Model now available.\n",
      "Started copying local path model/model.pb to hdfs path hdfs://10.0.2.15:8020/Projects/fawfawdasd/Models/local_model_dir/12\n",
      "\n",
      "Finished copying\n",
      "\n",
      "Exported model local_model_dir as version 12 successfully.\n",
      "Polling local_model_dir version 12 for model availability.\n",
      "Model now available.\n",
      "Started copying local path /srv/hops/hopsdata/tmp/nm-local-dir/usercache/NLucCipWZt56uigje1cP3j3_fYyvpEcQ7n_WV6vHxY8/appcache/application_1571991823790_0002/container_e03_1571991823790_0002_01_000001/model/model.pb to hdfs path hdfs://10.0.2.15:8020/Projects/fawfawdasd/Models/local_model_file/4\n",
      "\n",
      "Finished copying\n",
      "\n",
      "Exported model local_model_file as version 4 successfully.\n",
      "Polling local_model_file version 4 for model availability.\n",
      "Model now available."
     ]
    }
   ],
   "source": [
    "local_model_dir = os.getcwd() + '/model'\n",
    "local_model_file = local_model_dir + '/model.pb'\n",
    "if not os.path.exists(local_model_dir):\n",
    "    os.mkdir(local_model_dir)\n",
    "    f = open(local_model_file, \"w\")\n",
    "    f.write(\"model\")\n",
    "    f.close()\n",
    "    \n",
    "model.export(local_model_dir, 'local_model_dir')\n",
    "model.export('model', 'local_model_dir')\n",
    "\n",
    "model.export(local_model_file, 'local_model_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.export(model_path_relative, \"IrisFlowerClassifier\", model_version=4, overwrite=True, metrics={'accuracy': \"not number\"})\n",
    "    assert False\n",
    "except AssertionError:\n",
    "    assert True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.export(model_path_abs, \"IrisFlowerClassifier\", model_version=4, overwrite=True, metrics={1337: \"0.5\"})\n",
    "    assert False\n",
    "except AssertionError:\n",
    "    assert True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hdfs.exists(\"Models/IrisFlowerClassifier/1/iris_knn.pkl\")\n",
    "assert hdfs.exists(\"Models/IrisFlowerClassifier/1/iris_flower_classifier.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'modelDTO', 'href': 'https://hopsworks0.logicalclocks.com:8181/hopsworks-api/api/project/1144/models/IrisFlowerClassifier_5', 'created': '2019-10-25T08:53:47.582', 'description': 'A collection of models for IrisFlowerClassifier', 'id': 'IrisFlowerClassifier_5', 'metrics': {'accuracy': '30'}, 'name': 'IrisFlowerClassifier', 'userFullName': 'Admin Admin', 'version': 5}"
     ]
    }
   ],
   "source": [
    "best_model = model.get_best_model(\"IrisFlowerClassifier\", 'accuracy', Metric.MAX)\n",
    "print(best_model)\n",
    "assert best_model['name'] == \"IrisFlowerClassifier\"\n",
    "assert best_model['version'] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'modelDTO', 'href': 'https://hopsworks0.logicalclocks.com:8181/hopsworks-api/api/project/1144/models/IrisFlowerClassifier_3', 'created': '2019-10-25T08:53:32.951', 'description': 'A collection of models for IrisFlowerClassifier', 'id': 'IrisFlowerClassifier_3', 'metrics': {'accuracy': '0.5'}, 'name': 'IrisFlowerClassifier', 'userFullName': 'Admin Admin', 'version': 3}"
     ]
    }
   ],
   "source": [
    "best_model = model.get_best_model(\"IrisFlowerClassifier\", 'accuracy', Metric.MIN)\n",
    "print(best_model)\n",
    "assert best_model['name'] == \"IrisFlowerClassifier\"\n",
    "assert best_model['version'] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    best_model = model.get_best_model(\"not_exist\", 'accuracy', Metric.MIN)\n",
    "    assert False\n",
    "except model.ModelNotFound:\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    best_model = model.get_best_model(\"IrisFlowerClassifier\", 'not_exist', Metric.MIN)\n",
    "    assert False\n",
    "except model.ModelNotFound:\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.get_model(\"mnist\", 3)\n",
    "    assert False\n",
    "except model.ModelNotFound:\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model mnist as version 2 successfully.\n",
      "Polling mnist version 2 for model availability.\n",
      "Model now available."
     ]
    }
   ],
   "source": [
    "model_path = \"Resources/mnist/\"\n",
    "model.export(model_path, \"mnist\", model_version=2, overwrite=True)\n",
    "assert hdfs.exists(\"Models/mnist/2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Serve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a serving for model IrisFlowerClassifier ...\n",
      "Serving for model IrisFlowerClassifier successfully created"
     ]
    }
   ],
   "source": [
    "script_path = \"Models/IrisFlowerClassifier/1/iris_flower_classifier.py\"\n",
    "serving.exists(\"IrisFlowerClassifier\")\n",
    "if serving.exists(\"IrisFlowerClassifier\"):\n",
    "    serving.delete(\"IrisFlowerClassifier\")\n",
    "serving.create_or_update(script_path, \"IrisFlowerClassifier\", serving_type=\"SKLEARN\", \n",
    "                                 model_version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert serving.exists(\"IrisFlowerClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a serving for model mnist ...\n",
      "Serving for model mnist successfully created"
     ]
    }
   ],
   "source": [
    "model_path = \"Models/mnist/2/\"\n",
    "if serving.exists(\"mnist\"):\n",
    "    serving.delete(\"mnist\")\n",
    "serving.create_or_update(model_path, \"mnist\", serving_type=\"TENSORFLOW\", \n",
    "                                 model_version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert serving.exists(\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Data Access Operations on Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert serving.get_id(\"IrisFlowerClassifier\") is not None\n",
    "assert serving.get_id(\"mnist\") is not None\n",
    "assert \"Models/IrisFlowerClassifier/1/iris_flower_classifier.py\" in serving.get_artifact_path(\"IrisFlowerClassifier\")\n",
    "assert \"Models/mnist/2/\" in serving.get_artifact_path(\"mnist\")\n",
    "assert serving.get_type(\"IrisFlowerClassifier\") == \"SKLEARN\"\n",
    "assert serving.get_type(\"mnist\") == \"TENSORFLOW\"\n",
    "assert serving.get_version(\"IrisFlowerClassifier\") == 1\n",
    "assert serving.get_version(\"mnist\") == 2\n",
    "assert serving.get_kafka_topic(\"IrisFlowerClassifier\") is not None\n",
    "assert serving.get_kafka_topic(\"mnist\") is not None\n",
    "assert serving.get_status(\"IrisFlowerClassifier\") == \"Stopped\"\n",
    "assert serving.get_status(\"mnist\") == \"Stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Start/Stop Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: IrisFlowerClassifier...\n",
      "Serving with name: IrisFlowerClassifier successfully started\n",
      "Starting serving with name: mnist...\n",
      "Serving with name: mnist successfully started"
     ]
    }
   ],
   "source": [
    "serving.start(\"IrisFlowerClassifier\")\n",
    "serving.start(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert serving.get_status(\"IrisFlowerClassifier\") == \"Running\"\n",
    "assert serving.get_status(\"mnist\") == \"Running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping serving with name: IrisFlowerClassifier...\n",
      "Serving with name: IrisFlowerClassifier successfully stopped\n",
      "Stopping serving with name: mnist...\n",
      "Serving with name: mnist successfully stopped"
     ]
    }
   ],
   "source": [
    "serving.stop(\"IrisFlowerClassifier\")\n",
    "serving.stop(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert serving.get_status(\"IrisFlowerClassifier\") == \"Stopped\"\n",
    "assert serving.get_status(\"mnist\") == \"Stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Send Inference Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: IrisFlowerClassifier...\n",
      "Serving with name: IrisFlowerClassifier successfully started\n",
      "Starting serving with name: mnist...\n",
      "Serving with name: mnist successfully started"
     ]
    }
   ],
   "source": [
    "serving.start(\"IrisFlowerClassifier\")\n",
    "serving.start(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    data = {\"inputs\" : [[random.uniform(1, 8) for i in range(4)]]}\n",
    "    response = serving.make_inference_request(\"IrisFlowerClassifier\", data)\n",
    "    print(response)\n",
    "    assert response is not None\n",
    "    assert \"predictions\" or \"prediction\" in response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[0.00164127175, 5.96660321e-09, 0.421270937, 0.252333134, 9.5285659e-06, 0.319685638, 0.00119336753, 6.79556761e-05, 0.00376569643, 3.24859611e-05]]}\n",
      "{'predictions': [[0.000162672572, 1.6033546e-08, 0.813967884, 0.0811143368, 1.34863186e-07, 0.0978580043, 0.000632441486, 1.0146463e-05, 0.006224351, 2.99392887e-05]]}\n",
      "{'predictions': [[0.000340557075, 2.91633118e-09, 0.284717, 0.415190637, 1.27425676e-06, 0.267949641, 0.00123108109, 6.81755228e-06, 0.0304695051, 9.34484488e-05]]}\n",
      "{'predictions': [[0.000192859312, 4.40479786e-09, 0.145375639, 0.404690772, 8.73659829e-08, 0.444295526, 5.80828237e-05, 1.76897502e-05, 0.00536274444, 6.5450763e-06]]}\n",
      "{'predictions': [[2.72578454e-05, 7.65436781e-09, 0.104631409, 0.843327582, 5.98233146e-06, 0.0425032564, 0.000137176597, 7.24815836e-05, 0.00917834789, 0.0001165957]]}\n",
      "{'predictions': [[8.40702487e-05, 2.64554512e-08, 0.0373441279, 0.18434687, 2.55491286e-05, 0.713262379, 0.000958068122, 0.000318706181, 0.0630112737, 0.000648908317]]}\n",
      "{'predictions': [[0.00014561227, 3.7197061e-09, 0.060533423, 0.083192952, 9.29483747e-07, 0.849520683, 0.000234551204, 1.69306604e-05, 0.00631921832, 3.56524251e-05]]}\n",
      "{'predictions': [[3.72935756e-05, 8.63017324e-10, 0.124375015, 0.834977, 3.65689e-07, 0.039749328, 9.60409761e-06, 4.23410493e-05, 0.000804483716, 4.62059825e-06]]}\n",
      "{'predictions': [[0.000306829141, 7.32988159e-09, 0.16107291, 0.0682492107, 3.00146462e-06, 0.737307, 0.000461161544, 1.9087598e-05, 0.0324389711, 0.000141872486]]}\n",
      "{'predictions': [[0.000749515661, 6.03960881e-09, 0.1467053, 0.661955178, 8.44516705e-08, 0.162277579, 0.000869445445, 2.0538e-05, 0.027243251, 0.000179078328]]}\n",
      "{'predictions': [[0.000266171177, 1.31732483e-08, 0.47771, 0.475968093, 8.75346373e-08, 0.0419937186, 0.000320323132, 5.82237153e-05, 0.00364604616, 3.72183167e-05]]}\n",
      "{'predictions': [[0.000238965644, 7.2238695e-09, 0.625303924, 0.327854931, 9.17194939e-07, 0.0394092798, 0.000226255142, 1.95472676e-05, 0.0069060144, 4.01563084e-05]]}\n",
      "{'predictions': [[4.74557892e-05, 9.60593471e-09, 0.0190306678, 0.474656612, 2.65402548e-07, 0.50501883, 2.39801666e-05, 4.88213373e-05, 0.00108850398, 8.48142954e-05]]}\n",
      "{'predictions': [[6.82792743e-05, 7.27973959e-10, 0.0115543865, 0.850809216, 1.74524075e-08, 0.135988683, 4.54115943e-05, 3.9617611e-05, 0.00147068128, 2.37393833e-05]]}\n",
      "{'predictions': [[0.00055190007, 7.7864e-09, 0.00883444864, 0.094608359, 9.52262667e-07, 0.878285289, 0.000197626418, 8.7006767e-05, 0.0173218139, 0.000112558075]]}\n",
      "{'predictions': [[0.000173933411, 3.41250361e-09, 0.287635595, 0.153139234, 4.47332667e-07, 0.553207278, 0.000747859653, 1.59490592e-05, 0.00505796308, 2.17834659e-05]]}\n",
      "{'predictions': [[0.000733994355, 7.35839278e-10, 0.0533527546, 0.311084747, 4.41044449e-07, 0.628599346, 4.94232409e-05, 5.81754721e-05, 0.00609638, 2.47136832e-05]]}\n",
      "{'predictions': [[0.000222679213, 5.08270404e-09, 0.0579652712, 0.849162757, 2.53777387e-07, 0.0890633613, 0.000130271321, 3.70271155e-05, 0.00337463059, 4.36601513e-05]]}\n",
      "{'predictions': [[8.01627248e-05, 1.00924229e-08, 0.322018921, 0.585615158, 9.13855374e-08, 0.0877793431, 0.00013041473, 4.52944914e-05, 0.00432601245, 4.65439962e-06]]}\n",
      "{'predictions': [[0.000803725037, 1.09890843e-08, 0.0842310339, 0.514179528, 1.13682222e-06, 0.389087379, 0.000630521448, 0.000139466487, 0.0104281297, 0.000499093]]}"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    data = {\n",
    "                \"signature_name\": 'predict_images',\n",
    "                \"instances\": [np.random.rand(784).tolist()]\n",
    "            }\n",
    "    response = serving.make_inference_request(\"mnist\", data)\n",
    "    print(response)\n",
    "    assert response is not None\n",
    "    assert \"predictions\" in response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Kafka Inference Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro Python is only supported in python 2\n",
    "if sys.version_info[0] < 3:\n",
    "    topic = serving.get_kafka_topic(\"IrisFlowerClassifier\")\n",
    "    config = kafka.get_kafka_default_config()\n",
    "    config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "    consumer = Consumer(config)\n",
    "    topics = [topic]\n",
    "    consumer.subscribe(topics)\n",
    "    json_schema = kafka.get_schema(topic)\n",
    "    avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro Python is only supported in python 2\n",
    "if sys.version_info[0] < 3:\n",
    "    for i in range(0, 10):\n",
    "        msg = consumer.poll(timeout=1.5)\n",
    "        if msg is not None:\n",
    "            value = msg.value()\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            assert \"modelName\" in event_dict\n",
    "            assert \"requestTimestamp\" in event_dict\n",
    "            assert \"servingType\" in event_dict\n",
    "            assert \"inferenceResponse\" in event_dict\n",
    "            assert event_dict[\"modelName\"] == \"IrisFlowerClassifier\"\n",
    "            assert event_dict[\"servingType\"] == \"SKLEARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro Python is only supported in python 2\n",
    "if sys.version_info[0] < 3:\n",
    "    topic = serving.get_kafka_topic(\"mnist\")\n",
    "    config = kafka.get_kafka_default_config()\n",
    "    config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "    consumer = Consumer(config)\n",
    "    topics = [topic]\n",
    "    consumer.subscribe(topics)\n",
    "    json_schema = kafka.get_schema(topic)\n",
    "    avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro Python is only supported in python 2\n",
    "if sys.version_info[0] < 3:\n",
    "    for i in range(0, 10):\n",
    "        msg = consumer.poll(timeout=1.5)\n",
    "        if msg is not None:\n",
    "            value = msg.value()\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            assert \"modelName\" in event_dict\n",
    "            assert \"requestTimestamp\" in event_dict\n",
    "            assert \"servingType\" in event_dict\n",
    "            assert \"inferenceResponse\" in event_dict\n",
    "            assert event_dict[\"modelName\"] == \"mnist\"\n",
    "            assert event_dict[\"servingType\"] == \"TENSORFLOW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Delete Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting serving with name: IrisFlowerClassifier...\n",
      "Serving with name: IrisFlowerClassifier successfully deleted\n",
      "Deleting serving with name: mnist...\n",
      "Serving with name: mnist successfully deleted"
     ]
    }
   ],
   "source": [
    "serving.delete(\"IrisFlowerClassifier\")\n",
    "serving.delete(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not serving.exists(\"IrisFlowerClassifier\")\n",
    "assert not serving.exists(\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas and Numpy helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import pandas_helper as pandas\n",
    "import pandas as pd\n",
    "\n",
    "lst = ['Geeks', 'For', 'Geeks', 'is', 'portal', 'for', 'Geeks']\n",
    "\n",
    "data = {'Name':['Tom', 'nick', 'krish', 'jack'], 'Age':[20, 21, 19, 18]}\n",
    "\n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'],\n",
    "        'Age':[27, 24, 22, 32],\n",
    "        'Address':['Delhi', 'Kanpur', 'Allahabad', 'Kannauj'],\n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd']}\n",
    "\n",
    "pandas_df =  pd.DataFrame(data)\n",
    "pandas.write_csv(\"Resources/team-pandas.csv\", pandas_df)\n",
    "pandas.write_parquet(\"Resources/team-pandas.parquet\", pandas_df)\n",
    "pandas.write_json(\"Resources/team-pandas.json\", pandas_df)\n",
    "\n",
    "test_df = pandas.read_csv(\"Resources/team-pandas.csv\")\n",
    "test_df.count()\n",
    "\n",
    "test_df = pandas.read_json(\"Resources/team-pandas.json\")\n",
    "test_df.count()\n",
    "\n",
    "#test_df = pandas.read_parquet(\"Resources/team-pandas.parquet\")\n",
    "#test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import numpy_helper as numpy\n",
    "import numpy as np\n",
    "\n",
    "numpy_df = np.array([1, 2, 3])\n",
    "x = np.arange(10)\n",
    "\n",
    "numpy_df.shape\n",
    "\n",
    "numpy.save(\"Resources/numpy-path.npy\", numpy_df)\n",
    "numpy.savez(\"Resources/numpy-path.npz\", numpy_df, x )\n",
    "numpy.savez(\"Resources/numpy-compressed.npz\", numpy_df, x )\n",
    "\n",
    "npzfile = numpy.load(\"Resources/numpy-path.npz\")\n",
    "npzfile.files\n",
    "\n",
    "compressed = numpy.load(\"Resources/numpy-compressed.npz\")\n",
    "compressed.files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}